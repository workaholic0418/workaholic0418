{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rock_scissor_paper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1KbCLbC37-d3MuaT73UgHCC9voTMH6r6d",
      "authorship_tag": "ABX9TyMi69cgbjUzJxm3oAbfTgcT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raziel-JKM/Practice/blob/main/ai/EX-1/rock_scissor_paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr2r5YRC6ptK",
        "outputId": "69755e85-3eeb-48e4-f2fa-d31a9a18dc9f"
      },
      "source": [
        "from PIL import Image\n",
        "import os, glob\n",
        "\n",
        "print(\"PIL 라이브러리 import 완료!\")"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PIL 라이브러리 import 완료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmHCBjhX7d5C",
        "outputId": "c52d60ed-7ac2-46f4-d182-03932af67954"
      },
      "source": [
        "#이미지의 크기가 28x28  변경\n",
        "import os\n",
        "\n",
        "def resize_images(img_path):\n",
        "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
        "    \n",
        "\tprint(len(images), \" images to be resized.\")\n",
        "\n",
        "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
        "\ttarget_size=(28,28)\n",
        "\tfor img in images:\n",
        "\t\told_img=Image.open(img)\n",
        "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
        "\t\tnew_img.save(img, \"JPEG\")\n",
        "    \n",
        "\tprint(len(images), \" images resized.\")\n",
        "\t\n",
        "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
        "image_dir_path1 = \"/content/drive/MyDrive/Data/train/scissor\"\n",
        "#image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/scissor\"\n",
        "resize_images(image_dir_path1)\n",
        "\n",
        "print(\"가위 이미지 resize 완료!\")\n",
        "\n",
        "# 바위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
        "image_dir_path2 = \"/content/drive/MyDrive/Data/train/rock\"\n",
        "resize_images(image_dir_path2)\n",
        "\n",
        "print(\"바위 이미지 resize 완료!\")\n",
        "\n",
        "# 보 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
        "image_dir_path3 = \"/content/drive/MyDrive/Data/train/paper\"\n",
        "resize_images(image_dir_path3)\n",
        "\n",
        "print(\"보 이미지 resize 완료!\")"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400  images to be resized.\n",
            "400  images resized.\n",
            "가위 이미지 resize 완료!\n",
            "400  images to be resized.\n",
            "400  images resized.\n",
            "바위 이미지 resize 완료!\n",
            "400  images to be resized.\n",
            "400  images resized.\n",
            "보 이미지 resize 완료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKKtBXxCDlzm"
      },
      "source": [
        "#가위바위보의 경우 3개의 클래스 즉, 가위: 0, 바위: 1, 보: 2 로 라벨링\n",
        "import numpy as np\n",
        "\n",
        "def load_data(img_path, number_of_data=1600):  # 가위바위보 이미지 개수 총합에 주의하세요.\n",
        "    # 가위 : 0, 바위 : 1, 보 : 2\n",
        "    img_size=28\n",
        "    color=3\n",
        "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
        "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
        "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
        "\n",
        "    idx=0\n",
        "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
        "        img = np.array(Image.open(file),dtype=np.int32)\n",
        "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
        "        labels[idx]=0   # 가위 : 0\n",
        "        idx=idx+1\n",
        "\n",
        "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
        "        img = np.array(Image.open(file),dtype=np.int32)\n",
        "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
        "        labels[idx]=1   # 바위 : 1\n",
        "        idx=idx+1  \n",
        "    \n",
        "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
        "        img = np.array(Image.open(file),dtype=np.int32)\n",
        "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
        "        labels[idx]=2   # 보 : 2\n",
        "        idx=idx+1\n",
        "        \n",
        "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
        "    return imgs, labels\n"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmHZi9P-7pya",
        "outputId": "e548fea9-41f8-45eb-e4fb-3c222812cd21"
      },
      "source": [
        "\n",
        "image_dir_path = \"/content/drive/MyDrive/Data/train/\"\n",
        "(x_train, y_train)=load_data(image_dir_path)\n",
        "x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
        "\n",
        "print(\"x_train shape: {}\".format(x_train.shape))\n",
        "print(\"y_train shape: {}\".format(y_train.shape))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습데이터(x_train)의 이미지 개수는 1200 입니다.\n",
            "x_train shape: (1600, 28, 28, 3)\n",
            "y_train shape: (1600,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "ca3C3Bfj7zAK",
        "outputId": "4a0a5c28-4822-41e1-c674-d72371abf199"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(x_train[0])\n",
        "print('라벨: ', y_train[0])"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "라벨:  0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoFeZXxL8lAd",
        "outputId": "db040355-5c2c-42dc-e6ca-5682e34c8267"
      },
      "source": [
        "print(x_train.shape)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1600, 28, 28, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXeJHvt3GgO4",
        "outputId": "3b048315-1116-44b3-9a31-dbb3f9e539e6"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#train 데이터를 훈련셋 검증셋 80%, 20%로 랜덤하게 변경\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train_norm, y_train, test_size=0.2, shuffle=True, random_state=12)\n",
        "#x_split_train, x_split_val, y_split_train, y_split_val = train_test_split(x_train_norm, y_train, test_size=0.2, shuffle=True, random_state=12)\n",
        "# train, validation data의 사이즈 확인\n",
        "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1280, 28, 28, 3) (320, 28, 28, 3) (1280,) (320,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsuu2lIu71k5"
      },
      "source": [
        "#네트워크 설계"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVyFftgB705r",
        "outputId": "1d302b98-939b-483b-f210-6a1f37d25bfa"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "n_channel_1=16\n",
        "n_channel_2=32\n",
        "n_dense=32\n",
        "n_train_epoch=10\n",
        "\n",
        "model=keras.models.Sequential()\n",
        "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)))\n",
        "model.add(keras.layers.MaxPool2D(2,2))\n",
        "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
        "model.add(keras.layers.MaxPooling2D((2,2)))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
        "model.add(keras.layers.Dense(3, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "model.compile(optimizer='adam',\n",
        "             loss='sparse_categorical_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_22 (Conv2D)           (None, 26, 26, 16)        448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 13, 13, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 11, 11, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 32)                25632     \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 3)                 99        \n",
            "=================================================================\n",
            "Total params: 30,819\n",
            "Trainable params: 30,819\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_22 (Conv2D)           (None, 26, 26, 16)        448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 13, 13, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 11, 11, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 32)                25632     \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 3)                 99        \n",
            "=================================================================\n",
            "Total params: 30,819\n",
            "Trainable params: 30,819\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCjo5sYjNeZ5",
        "outputId": "a44aa0ec-91d9-4e1d-f6a7-2c5c3f383a75"
      },
      "source": [
        "# 모델 훈련\n",
        "result = model.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "40/40 [==============================] - 2s 28ms/step - loss: 1.0170 - accuracy: 0.5688 - val_loss: 0.8179 - val_accuracy: 0.4844\n",
            "Epoch 2/20\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.6282 - accuracy: 0.7547 - val_loss: 0.4633 - val_accuracy: 0.8438\n",
            "Epoch 3/20\n",
            "40/40 [==============================] - 1s 26ms/step - loss: 0.3980 - accuracy: 0.8594 - val_loss: 0.3059 - val_accuracy: 0.8875\n",
            "Epoch 4/20\n",
            "40/40 [==============================] - 1s 25ms/step - loss: 0.2696 - accuracy: 0.9000 - val_loss: 0.2123 - val_accuracy: 0.9156\n",
            "Epoch 5/20\n",
            "40/40 [==============================] - 1s 25ms/step - loss: 0.1924 - accuracy: 0.9391 - val_loss: 0.1642 - val_accuracy: 0.9531\n",
            "Epoch 6/20\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1587 - accuracy: 0.9422 - val_loss: 0.1240 - val_accuracy: 0.9594\n",
            "Epoch 7/20\n",
            "40/40 [==============================] - 1s 26ms/step - loss: 0.1230 - accuracy: 0.9680 - val_loss: 0.1108 - val_accuracy: 0.9625\n",
            "Epoch 8/20\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0905 - accuracy: 0.9766 - val_loss: 0.0763 - val_accuracy: 0.9812\n",
            "Epoch 9/20\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0698 - accuracy: 0.9805 - val_loss: 0.0654 - val_accuracy: 0.9844\n",
            "Epoch 10/20\n",
            "40/40 [==============================] - 1s 25ms/step - loss: 0.0511 - accuracy: 0.9883 - val_loss: 0.0563 - val_accuracy: 0.9844\n",
            "Epoch 11/20\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0457 - accuracy: 0.9914 - val_loss: 0.0697 - val_accuracy: 0.9688\n",
            "Epoch 12/20\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0369 - accuracy: 0.9930 - val_loss: 0.0333 - val_accuracy: 0.9937\n",
            "Epoch 13/20\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0259 - accuracy: 0.9953 - val_loss: 0.0295 - val_accuracy: 0.9969\n",
            "Epoch 14/20\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0194 - accuracy: 0.9992 - val_loss: 0.0213 - val_accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0164 - accuracy: 0.9992 - val_loss: 0.0185 - val_accuracy: 0.9969\n",
            "Epoch 16/20\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0145 - accuracy: 0.9992 - val_loss: 0.0212 - val_accuracy: 0.9969\n",
            "Epoch 17/20\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0094 - accuracy: 0.9992 - val_loss: 0.0196 - val_accuracy: 0.9969\n",
            "Epoch 19/20\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0092 - val_accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kiEVVolHvJ5"
      },
      "source": [
        "\n",
        "# #모델 잘만들었는지 평가해보기\n",
        "# test_loss, test_accuracy = model.evaluate(x_test_norm,y_test, verbose=2)\n",
        "# print(\"test_loss: {} \".format(test_loss))\n",
        "# print(\"test_accuracy: {}\".format(test_accuracy))"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plYeF-ZGTkGA",
        "outputId": "bcaa69ca-2856-4529-f5c0-4c9eb2bab479"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   validation_split=0.2)\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory('/content/drive/MyDrive/Data/train',\n",
        "                                                 target_size = (28, 28),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical',subset='training')\n",
        "val_gen  = train_datagen.flow_from_directory('/content/drive/MyDrive/Data/train',\n",
        "                                                 target_size = (28 , 28),\n",
        "                                                  batch_size = 32,\n",
        "                                                 class_mode = 'categorical',subset='validation')"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 960 images belonging to 3 classes.\n",
            "Found 240 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6tW87TRPMuJ"
      },
      "source": [
        "def create_gen():\n",
        "    # 생성기 및 데이터 증강으로 이미지 로드\n",
        "    train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n",
        "        validation_split=0.1\n",
        "    )\n",
        "\n",
        "    test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "    )\n",
        "\n",
        "    train_images = train_generator.flow_from_dataframe(\n",
        "        dataframe=x_train,\n",
        "        x_col='Filepath', # 파일위치 열이름\n",
        "        y_col='Label', # 클래스 열이름\n",
        "        target_size=(28, 28), # 이미지 사이즈\n",
        "        color_mode='rgb', # 이미지 채널수\n",
        "        class_mode='categorical', # Y값(Label값)\n",
        "        batch_size=32,\n",
        "        shuffle=True, # 데이터를 섞을지 여부\n",
        "        seed=0,\n",
        "        subset='training', # train 인지 val인지 설정\n",
        "        rotation_range=30, # 회전제한 각도 30도\n",
        "        zoom_range=0.15, # 확대 축소 15%\n",
        "        width_shift_range=0.2, # 좌우이동 20%\n",
        "        height_shift_range=0.2, # 상하이동 20%\n",
        "        shear_range=0.15, # 반시계방햐의 각도\n",
        "        horizontal_flip=True, # 좌우 반전 True\n",
        "        fill_mode=\"nearest\"\n",
        "        # 이미지 변경시 보완 방법 (constant, nearest, reflect, wrap) 4개 존재\n",
        "    )\n",
        "\n",
        "    val_images = train_generator.flow_from_dataframe(\n",
        "        dataframe=x_train,\n",
        "        x_col='Filepath',\n",
        "        y_col='Label',\n",
        "        target_size=(28, 28),\n",
        "        color_mode='rgb',\n",
        "        class_mode='categorical',\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "        seed=0,\n",
        "        subset='validation',\n",
        "        rotation_range=30,\n",
        "        zoom_range=0.15,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.15,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode=\"nearest\"\n",
        "    )\n",
        "\n",
        "    test_images = test_generator.flow_from_dataframe(\n",
        "        dataframe=x_train,\n",
        "        x_col='Filepath',\n",
        "        y_col='Label',\n",
        "        target_size=(28, 28),\n",
        "        color_mode='rgb',\n",
        "        class_mode='categorical',\n",
        "        batch_size=32,\n",
        "        shuffle=False\n",
        "    )\n",
        "    \n",
        "    return train_generator,test_generator,train_images,val_images,test_images"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bdcs0qJcY4_K",
        "outputId": "ae3c3540-6486-4dd2-c84a-e76975f7c340"
      },
      "source": [
        "# Initialising the CNN\n",
        "cnn = tf.keras.models.Sequential()\n",
        "\n",
        "# Step 1 - Convolution\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[28, 28, 3]))\n",
        "\n",
        "# Step 2 - Pooling\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "# Adding convolutional layer\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "# Step 3 - Flattening\n",
        "cnn.add(tf.keras.layers.Flatten())\n",
        "\n",
        "# Step 4 - Full Connection\n",
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
        "\n",
        "# Step 5 - Output Layer\n",
        "cnn.add(tf.keras.layers.Dense(units=3, activation='softmax'))\n",
        "\n",
        "# Compiling the CNN\n",
        "cnn.compile(optimizer = 'adam', \n",
        "            loss = 'categorical_crossentropy', \n",
        "            metrics = ['accuracy'])\n",
        "cnn.summary()"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_24 (Conv2D)           (None, 26, 26, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 11, 11, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 128)               102528    \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 113,059\n",
            "Trainable params: 113,059\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Op2n45-KZAnq",
        "outputId": "1e1f53aa-3af7-4239-f89c-7e3a50a1a9f1"
      },
      "source": [
        "cnn.fit(x = train_gen, validation_data = val_gen, epochs = 10)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "30/30 [==============================] - 41s 90ms/step - loss: 1.0491 - accuracy: 0.4896 - val_loss: 0.8879 - val_accuracy: 0.9250\n",
            "Epoch 2/10\n",
            "30/30 [==============================] - 3s 91ms/step - loss: 0.7502 - accuracy: 0.7385 - val_loss: 0.4336 - val_accuracy: 0.8667\n",
            "Epoch 3/10\n",
            "30/30 [==============================] - 3s 87ms/step - loss: 0.4746 - accuracy: 0.8302 - val_loss: 0.2966 - val_accuracy: 0.9250\n",
            "Epoch 4/10\n",
            "30/30 [==============================] - 3s 90ms/step - loss: 0.2933 - accuracy: 0.8917 - val_loss: 0.1367 - val_accuracy: 0.9708\n",
            "Epoch 5/10\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 0.2095 - accuracy: 0.9323 - val_loss: 0.0885 - val_accuracy: 0.9875\n",
            "Epoch 6/10\n",
            "30/30 [==============================] - 3s 84ms/step - loss: 0.1636 - accuracy: 0.9448 - val_loss: 0.0723 - val_accuracy: 0.9792\n",
            "Epoch 7/10\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 0.1152 - accuracy: 0.9604 - val_loss: 0.0437 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "30/30 [==============================] - 3s 84ms/step - loss: 0.0781 - accuracy: 0.9802 - val_loss: 0.0441 - val_accuracy: 0.9917\n",
            "Epoch 9/10\n",
            "30/30 [==============================] - 3s 87ms/step - loss: 0.0716 - accuracy: 0.9802 - val_loss: 0.0239 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.0538 - accuracy: 0.9906 - val_loss: 0.0239 - val_accuracy: 0.9958\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f88ab056410>"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    }
  ]
}